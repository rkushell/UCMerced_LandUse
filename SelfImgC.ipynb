{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27d66573-fc62-4a16-a766-1bfcef8e0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30e881ef-b4b4-464d-acda-e2135e80acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = 'uniform' , sets the binnig for p+2 , i.e. 10 bins instead of p^2 i.e. 256\n",
    "# ravel converts the 2D lbp matrix to 1D Matrix , ie flattens it out\n",
    "# np.arange(0, n_bins+1) produces integers [0, 1, 2, ..., n_bins].\n",
    "# That means there are n_bins bins, each spanning a range like:\n",
    "# Bin 0 → [0,1)\n",
    "# Bin 1 → [1,2)\n",
    "# Bin n_bins-1 → [n_bins-1, n_bins)\n",
    "# So each possible LBP code (0 … n_bins-1) has its own bin\n",
    "# Normalize the histogram (astype)\n",
    "# Add a small epsilon to avoid division by zero, hist /= (hist.sum() + 1e-6)\n",
    "def extract_lbp_features(gray_image,radius=1,n_points=8):\n",
    "    lbp = local_binary_pattern(gray_image,n_points,radius,method='uniform')\n",
    "    n_bins = n_points+2\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_bins + 1), range=(0, n_bins))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b86e42c3-2d01-4dc8-97b5-d246ea708ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# levels → number of gray levels (controls matrix size).\n",
    "# symmetric=True → makes matrix undirected by counting (i, j) and (j, i) together.\n",
    "# normed=True → converts counts into probabilities.\n",
    "def extract_glcm_features(gray_image):\n",
    "    distances = [1,3,5]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    glcm = graycomatrix(gray_image,distances = distances,angles = angles, levels = 256, symmetric=True,normed=True)\n",
    "    contrast = graycoprops(glcm,'contrast').mean()\n",
    "    dissimilarity = graycoprops(glcm,'dissimilarity').mean()\n",
    "    homogeneity = graycoprops(glcm,'homogeneity').mean()\n",
    "    energy = graycoprops(glcm,'energy').mean()\n",
    "    correlation = graycoprops(glcm,'correlation').mean()\n",
    "    asm = graycoprops(glcm,'ASM').mean()\n",
    "\n",
    "    return [contrast,dissimilarity,homogeneity,energy,correlation,asm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76253fb4-13ae-47bd-9824-827e76048f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path extracted and lbp,glcm features extracted for each image and appended in features and labels columns resp.\n",
    "def load_data_and_extract_features(dataset_path):\n",
    "    features = []\n",
    "    labels = []\n",
    "    print(\"Feature extraction started ---\")\n",
    "    class_names = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(dataset_path,class_name)\n",
    "        image_files = [\n",
    "            f for f in os.listdir(class_path)\n",
    "            if f.lower().endswith((\".tif\"))\n",
    "        ]\n",
    "        print(f\"Processing class: {class_name} ({len(image_files)} images)\")\n",
    "        for image_name in image_files:\n",
    "            image_path = os.path.join(class_path,image_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Warning: Could not read image {image_path}. Skipping.\")\n",
    "                continue\n",
    "            image = cv2.resize(image,(256,256))\n",
    "            gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "            glcm_features = extract_glcm_features(gray_image)\n",
    "            lbp_features = extract_lbp_features(gray_image)\n",
    "            combined_features = glcm_features + lbp_features\n",
    "            features.append(combined_features)\n",
    "            labels.append(class_name)\n",
    "    print(\"Feature extraction completed.\")\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb22813c-7ee9-4495-92fd-e7ff16530a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction started ---\n",
      "Processing class: agricultural (100 images)\n",
      "Processing class: airplane (100 images)\n",
      "Processing class: baseballdiamond (100 images)\n",
      "Processing class: beach (100 images)\n",
      "Processing class: buildings (100 images)\n",
      "Processing class: harbor (100 images)\n",
      "Processing class: intersection (100 images)\n",
      "Processing class: mediumresidential (100 images)\n",
      "Processing class: mobilehomepark (100 images)\n",
      "Processing class: overpass (100 images)\n",
      "Processing class: parkinglot (100 images)\n",
      "Processing class: river (100 images)\n",
      "Processing class: runway (100 images)\n",
      "Processing class: sparseresidential (100 images)\n",
      "Processing class: storagetanks (100 images)\n",
      "Processing class: tenniscourt (100 images)\n",
      "Feature extraction completed.\n",
      "Total samples: 1600, Features per sample: 16\n",
      "Training set size: 1200\n",
      "Testing set size: 400\n",
      "\n",
      "Starting SVM hyperparameter tuning with GridSearchCV...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Tuning complete.\n",
      "Best parameters found:  {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "Evaluating the best model on the test set...\n",
      "Model Accuracy: 77.50%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"UCMerced_LandUse/Images\"\n",
    "    if not os.path.exists(dataset_path) or not os.path.isdir(dataset_path):\n",
    "        print(f\"Error: The directory '{dataset_path}' does not exist.\")\n",
    "        print(\"Please update the 'dataset_path' variable with the correct path to your image dataset.\")\n",
    "    else:\n",
    "        X,y = load_data_and_extract_features(dataset_path)\n",
    "        if X.shape[0] == 0:\n",
    "            print(\"Error: No data was loaded. Please check your dataset folder structure and contents.\")\n",
    "        else:\n",
    "            print(f\"Total samples: {X.shape[0]}, Features per sample: {X.shape[1]}\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X,y,test_size=0.25,random_state=42,stratify=y\n",
    "            )\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            print(f\"Training set size: {X_train_scaled.shape[0]}\")\n",
    "            print(f\"Testing set size: {X_test_scaled.shape[0]}\")\n",
    "            print(\"\\nStarting SVM hyperparameter tuning with GridSearchCV...\")\n",
    "            param_grid = {\n",
    "                'C':[0.1,1,10,100],\n",
    "                'gamma':[1,0.1,0.01,0.001,'scale'],\n",
    "                'kernel':['rbf']\n",
    "            }\n",
    "            grid_search = GridSearchCV(SVC(probability=True, random_state=42), param_grid, refit=True, verbose=2, cv=3, n_jobs=-1)\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "            print(\"Tuning complete.\")\n",
    "            print(\"Best parameters found: \", grid_search.best_params_)\n",
    "            svm_classifier = grid_search.best_estimator_\n",
    "            print(\"\\nEvaluating the best model on the test set...\")\n",
    "            y_pred = svm_classifier.predict(X_test_scaled)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228901bf-f4a0-496d-bf15-5d2ef072d1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
